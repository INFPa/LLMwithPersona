{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查CUDA是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取文件\n",
    "data_original = pd.read_excel('../input folder/公牛原帖_作者id合并后.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 处理源数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tips： 第一轮循环后，需要将data_original换成remaining_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一轮循环后，需要将data_original换成remaining_data\n",
    "split_data = data_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 随机抽样300条数据，交个GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tips: 每一轮的random_state应该变化，并且记录下seed的数字，方便复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（待选\n",
    "df_sampled = split_data.sample(n=300,random_state = 42)\n",
    "# 查看特定列\n",
    "df_content = df_sampled['作品正文内容']\n",
    "df_content.to_csv('ym-loop-0-RANDOM-seed42.txt', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 检查数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = split_data.columns.tolist()\n",
    "column_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 处理对应列的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗数据，删除重复值和空值\n",
    "clean_data = split_data.dropna(subset=['作品标题', '作品正文内容'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对评论进行分词处理\n",
    "## 加载停用词文件\n",
    "def load_stopwords(paths):\n",
    "    stopwords = set()\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            stopwords.update(line.strip() for line in file)\n",
    "    return stopwords\n",
    "stopwords_paths = [\"../中文停用词/cn_stopwords.txt\",\n",
    "                       \"../中文停用词/hit_stopwords.txt\",\n",
    "                       \"../中文停用词/scu_stopwords.txt\",\n",
    "                       \"../中文停用词/baidu_stopwords.txt\"]\n",
    "stopwords = load_stopwords(stopwords_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义分词和过滤函数\n",
    "def tokenize_and_filter(text, stopwords):\n",
    "    words = jieba.cut(text)\n",
    "    return [word for word in words if word not in stopwords and word.strip()]\n",
    "\n",
    "\n",
    "data_fenci = clean_data['作品正文内容'].apply(lambda x: tokenize_and_filter(x, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = data_fenci\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2 ：引入GPT生成的人群和特征词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引用参考人群聚类文档\n",
    "excel_path = \"../1029ym论文人群关键词.xlsx\"\n",
    "def read_dimensions_from_excel(excel_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    dimensions = {}\n",
    "    for column in df.columns:\n",
    "        dimensions[column] = df[column].dropna().tolist()\n",
    "    return dimensions\n",
    "# cluster_reference = pd.read_excel('../人群分词.xlsx')\n",
    "dimensions = read_dimensions_from_excel(excel_path)\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 引入腾讯近义词向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载腾讯AI实验室的词向量模型\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../Tencent Embedding/tencent-ailab-embedding-zh-d200-v0.2.0.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: 处理词向量以及相似度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 GPT中的特征词转换成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个新的字典，用于存储每个维度下特征词的向量张量\n",
    "dimensions_vectors = {}\n",
    "\n",
    "for dimension, keywords in dimensions.items():\n",
    "    # 存储当前维度下的所有特征词向量张量\n",
    "    vectors = []\n",
    "    for keyword in keywords:\n",
    "        # 检查词向量模型中是否有该词\n",
    "        if keyword in word_vectors.key_to_index:\n",
    "            # 将词向量转换为PyTorch张量，并加载到GPU\n",
    "            vector_tensor = torch.tensor(word_vectors[keyword], dtype=torch.float32, device=device)\n",
    "            vectors.append(vector_tensor)\n",
    "    # 将转换后的向量张量列表存储在新字典中\n",
    "    dimensions_vectors[dimension] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设计聚类的名称\n",
    "def read_cluster_names(excel_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    return df.columns.tolist()  # 假设聚类名在第一行\n",
    "cluster_names = read_cluster_names(excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 对每条数据进行打分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 对每条数据进行维度归纳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_similarity_with_threshold_and_words(comment_words, dimensions_vectors, word_vectors, threshold=0.55, device='cuda'):\n",
    "    \"\"\"\n",
    "    计算评论中每个词的相似度，使用阈值过滤不相关词，并进行加权计算，\n",
    "    返回每个评论的相似度分数（基于维度特征词的加权平均）和最相关的特征词。\n",
    "    \"\"\"\n",
    "    comment_vectors = [get_word_tensor(word, word_vectors, device) for word in comment_words if word in word_vectors]\n",
    "    if not comment_vectors:\n",
    "        return {}, []  # 返回空的相似度和空的词列表\n",
    "\n",
    "    comment_matrix = torch.stack(comment_vectors)\n",
    "    scores = {}\n",
    "    related_words = []\n",
    "\n",
    "    for dimension, feature_vectors in dimensions_vectors.items():\n",
    "        total_weight = 0.0\n",
    "        weighted_similarity_sum = 0.0\n",
    "        word_similarities = []\n",
    "\n",
    "        for fv in feature_vectors:\n",
    "            sims = torch.stack([F.cosine_similarity(comment_matrix, fv.unsqueeze(0).expand_as(comment_matrix), dim=1)])\n",
    "            similarity = sims.max().item() if sims.numel() > 0 else 0\n",
    "\n",
    "            # 如果相似度大于阈值，进行加权\n",
    "            if similarity > threshold:\n",
    "                weight = similarity\n",
    "                weighted_similarity_sum += similarity * weight\n",
    "                total_weight += weight\n",
    "                word_similarities.append((fv, similarity))  # 存储词向量和相似度\n",
    "\n",
    "        if total_weight > 0:\n",
    "            average_similarity = weighted_similarity_sum / total_weight\n",
    "            scores[dimension] = average_similarity\n",
    "\n",
    "            # 按照相似度降序排序，并选择前三个词\n",
    "            word_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_words = [word_vectors.index_to_key[word_similarities[i][0].tolist()] for i in range(min(3, len(word_similarities)))]\n",
    "            related_words.append(top_words)\n",
    "        else:\n",
    "            scores[dimension] = 0\n",
    "            related_words.append([])\n",
    "\n",
    "    return scores, related_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每条评论的相似度得分和最相关的词\n",
    "similarity_scores_subset = comments.apply(lambda x: calculate_weighted_similarity_with_threshold_and_words(x, dimensions_vectors, word_vectors, device='cuda', threshold=0.5))\n",
    "\n",
    "# 转化成DataFrame，存储相似度得分\n",
    "similarity_scores_df = pd.DataFrame([score[0] for score in similarity_scores_subset.tolist()])\n",
    "\n",
    "# 提取每条评论最相关的词\n",
    "related_words_subset = [score[1] for score in similarity_scores_subset.tolist()]\n",
    "related_words_df = pd.DataFrame(related_words_subset, columns=[\"Top_1\", \"Top_2\", \"Top_3\"])\n",
    "\n",
    "# 计算每条评论的最大相似度得分\n",
    "max_similarity_scores = similarity_scores_df.max(axis=1)\n",
    "\n",
    "# 设置阈值\n",
    "threshold = 0.55\n",
    "\n",
    "# 基于阈值筛选评论，得到符合条件的评论索引\n",
    "filtered_indexes = max_similarity_scores[max_similarity_scores > threshold].index\n",
    "filtered_similarity_scores_df = similarity_scores_df.loc[filtered_indexes]\n",
    "filtered_related_words_df = related_words_df.loc[filtered_indexes]\n",
    "\n",
    "# 选择每个评论最高分的维度（每条评论与哪个主题最相关）\n",
    "scores_max = filtered_similarity_scores_df.max(axis=1)\n",
    "max_scores = filtered_similarity_scores_df.idxmax(axis=1)\n",
    "\n",
    "# 输出原始功能的结果\n",
    "print(\"\\nFiltered Similarity Scores:\")\n",
    "print(filtered_similarity_scores_df)\n",
    "\n",
    "print(\"\\nFiltered Related Words (Top 3):\")\n",
    "print(filtered_related_words_df)\n",
    "\n",
    "print(\"\\nMax Scores per Comment:\")\n",
    "print(scores_max)\n",
    "\n",
    "print(\"\\nMax Scores (Topic) per Comment:\")\n",
    "print(max_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每条评论的相似度得分\n",
    "similarity_scores_subset = comments.apply(lambda x: calculate_weighted_similarity_with_threshold_and_words(x, dimensions_vectors, word_vectors, device='cuda', threshold=0.5))\n",
    "\n",
    "# 转化成DataFrame\n",
    "similarity_scores_df = pd.DataFrame(similarity_scores_subset.tolist())\n",
    "\n",
    "# 计算每条评论的最大相似度得分\n",
    "max_similarity_scores = similarity_scores_df.max(axis=1)\n",
    "\n",
    "# 设置阈值\n",
    "threshold = 0.55\n",
    "\n",
    "# 基于阈值筛选评论\n",
    "filtered_indexes = max_similarity_scores[max_similarity_scores > threshold].index\n",
    "filtered_similarity_scores_df = similarity_scores_df.loc[filtered_indexes]\n",
    "\n",
    "# 选择每个评论最高分的维度\n",
    "scores_max = filtered_similarity_scores_df.max(axis=1)\n",
    "max_scores = filtered_similarity_scores_df.idxmax(axis=1)\n",
    "\n",
    "# 输出原始功能的结果\n",
    "print(\"\\nFiltered Similarity Scores:\")\n",
    "print(filtered_similarity_scores_df)\n",
    "print(\"\\nMax Scores per Comment:\")\n",
    "print(scores_max)\n",
    "print(\"\\nMax Scores (Topic) per Comment:\")\n",
    "print(max_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 针对每组数据计算，并制作citespace里要用到的图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取每条评论的相似度得分和与主题相关的前三个词\n",
    "def get_top_words_for_comments(comments, dimensions_vectors, word_vectors, device='cuda', threshold=0.55):\n",
    "    top_words_per_comment = []\n",
    "    similarity_scores_per_comment = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        scores, top_words = calculate_weighted_similarity_with_threshold_and_words(comment, dimensions_vectors, word_vectors, threshold=threshold, device=device)\n",
    "        top_words_per_comment.append(top_words)\n",
    "        similarity_scores_per_comment.append(scores)\n",
    "    \n",
    "    return top_words_per_comment, similarity_scores_per_comment\n",
    "\n",
    "top_words_per_comment, similarity_scores_per_comment = get_top_words_for_comments(comments, dimensions_vectors, word_vectors, device='cuda', threshold=0.55)\n",
    "\n",
    "# 输出与每个评论相关的前三个词\n",
    "for idx, top_words in enumerate(top_words_per_comment):\n",
    "    print(f\"Comment {idx + 1}:\")\n",
    "    print(\"Top related words:\", top_words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用计算和逆向映射函数\n",
    "similarity_scores_subset, top_related_words = extract_top_related_words_for_comments(\n",
    "    comments, dimensions_vectors, word_vectors, device='cuda', threshold=0.55, top_n=3\n",
    ")\n",
    "\n",
    "# 展平 top_related_words 中的词列表\n",
    "top_related_words_flat = [word for sublist in top_related_words for word in sublist]\n",
    "\n",
    "# 调用逆向映射函数，获取最终的相关词\n",
    "final_related_words = reverse_mapping_to_original_words(top_related_words_flat, comments[0], word_vectors, threshold=0.55)\n",
    "\n",
    "print(f\"Final Related Words: {final_related_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top Related Words (Before Reverse Mapping): {top_related_words}\")\n",
    "print(f\"Comments: {comments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合数据，生成CiteSpace所需的格式\n",
    "citespace_data = []\n",
    "for index, row in top_related_words_df.iterrows():\n",
    "    comment_similarity_scores = similarity_scores_df.iloc[index].to_dict()\n",
    "    for word in row:\n",
    "        word_info = {\n",
    "            \"Keyword\": word, \n",
    "            \"Frequency\": row.value_counts().get(word, 0),  # 计算频率\n",
    "            \"Related Dimension\": comment_similarity_scores,  # 维度的相关性\n",
    "        }\n",
    "        citespace_data.append(word_info)\n",
    "\n",
    "# 转换为DataFrame\n",
    "citespace_df = pd.DataFrame(citespace_data)\n",
    "\n",
    "# 保存为csv格式，供CiteSpace使用\n",
    "citespace_df.to_csv('citespace_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4：导出相应的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 已经筛选出来的数据，以及还有多少数据未被筛选出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取不符合条件的数据\n",
    "remaining_indexes = max_similarity_scores[max_similarity_scores <= threshold].index\n",
    "remaining_similarity_scores_df = similarity_scores_df.loc[remaining_indexes]\n",
    "\n",
    "# 获取筛选出的数据和未筛选的数据\n",
    "filtered_data = comments.loc[filtered_indexes]\n",
    "remaining_data = comments.loc[remaining_indexes]\n",
    "\n",
    "# 打印未筛选数据的数量\n",
    "print(f\"未筛选数据的数量：{len(remaining_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 将已经筛选出来的数据保存到datafarame中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一轮循环\n",
    "# 保存已筛选的数据到DataFrame\n",
    "filtered_data_df = pd.DataFrame({\n",
    "    'Comment': filtered_data,\n",
    "    'Max Similarity Score': max_similarity_scores.loc[filtered_indexes],  # 最大相似度得分\n",
    "    'Matching Dimension': filtered_similarity_scores_df.idxmax(axis=1)  # 匹配的主题维度\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一轮循环\n",
    "# 导出筛选后的数据到Excel\n",
    "filtered_data_df.to_excel('../output_folder/筛选后的数据.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "# 后续循环\n",
    "# 1. 构造新的 DataFrame\n",
    "new_filtered_data_df = pd.DataFrame({\n",
    "    'Comment': filtered_data,\n",
    "    'Max Similarity Score': max_similarity_scores.loc[filtered_indexes],  # 最大相似度得分\n",
    "    'Matching Dimension': filtered_similarity_scores_df.idxmax(axis=1)\n",
    "})\n",
    "\n",
    "# 2. 读取现有的 Excel 文件\n",
    "excel_path = '../output_folder/筛选后的数据.xlsx'\n",
    "\n",
    "try:\n",
    "    # 尝试读取现有的文件\n",
    "    book = load_workbook(excel_path)\n",
    "except FileNotFoundError:\n",
    "    # 如果文件不存在，创建一个新的文件\n",
    "    book = None\n",
    "\n",
    "# 3. 使用 ExcelWriter 添加到 Sheet2\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    if book:\n",
    "        # 如果文件已经存在，加载现有的工作簿\n",
    "        writer.book = book\n",
    "        \n",
    "    # 将新数据添加到 Sheet2 中（第几轮循环就写Sheet X 号）\n",
    "    new_filtered_data_df.to_excel(writer, index=False, sheet_name='Sheet2')\n",
    "\n",
    "    # 打印输出确认\n",
    "    print(f\"新数据已成功添加到 {excel_path} 的 Sheet2 中\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 导出本次循环中对应人群的帖子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定保存文件的基础路径\n",
    "base_path = '../output_folder/已经筛选出的人群的帖子'\n",
    "\n",
    "# 检查并创建基础路径\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "# 按 'Matching Dimension' 进行分组\n",
    "for dimension, group_df in filtered_data_df.groupby('Matching Dimension'):\n",
    "    # 构建文件名\n",
    "    filename = f\"主题_{dimension}.xlsx\"\n",
    "    file_path = os.path.join(base_path, filename)\n",
    "    \n",
    "    # 保存到Excel文件\n",
    "    group_df.to_excel(file_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {dimension} posts to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 保存以及导出citespace所需要的网络图数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一轮循环\n",
    "excel_path = '../output_folder/citespace_network_data.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    # 保存节点数据到 'Nodes' 工作表\n",
    "    nodes_df.to_excel(writer, index=False, sheet_name='Nodes')\n",
    "    \n",
    "    # 保存边数据到 'Edges' 工作表\n",
    "    edges_df.to_excel(writer, index=False, sheet_name='Edges')\n",
    "    \n",
    "    print(f\"节点和边数据已保存到 {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后续循环\n",
    "# Excel 文件路径\n",
    "excel_path = '../output_folder/citespace_network_data.xlsx'\n",
    "\n",
    "# 读取现有的工作簿\n",
    "try:\n",
    "    # 尝试加载现有文件\n",
    "    book = load_workbook(excel_path)\n",
    "except FileNotFoundError:\n",
    "    # 如果文件不存在，创建一个新的文件\n",
    "    book = None\n",
    "\n",
    "# 使用 ExcelWriter 将数据追加到现有文件\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    if book:\n",
    "        # 如果文件已存在，加载工作簿并将其赋给 writer\n",
    "        writer.book = book\n",
    "\n",
    "    # 将新数据追加到 'Nodes' 工作表（如果已存在，则会在现有数据后追加）\n",
    "    nodes_df.to_excel(writer, index=False, sheet_name='Nodes', header=not book)\n",
    "\n",
    "    # 将新数据追加到 'Edges' 工作表（如果已存在，则会在现有数据后追加）\n",
    "    edges_df.to_excel(writer, index=False, sheet_name='Edges', header=not book)\n",
    "\n",
    "    print(f\"新数据已追加到 {excel_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
